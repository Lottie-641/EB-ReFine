{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee57dc1-04cb-4448-8903-ee377e09c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_semantic_dir(self) -> Path:\n",
    "    base_dir = Path(\"semantic_data\") / self.__log_name\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f5a20-5d38-4d1b-ac0c-271a79e7290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = self._ensure_semantic_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55f695-793f-4300-a905-c5e625097489",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = base_dir / f\"{self.__log_name}_{obj_type}_{self.__setting}.pkl\"\n",
    "os.makedirs(f\"semantic_data/{self.__log_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af5256-8133-4d11-bf23-6ae35001a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from neural_network.HistoryDataset import CustomDataset\n",
    "from neural_network.llamp_multiout import BertMultiOutputClassificationHeads\n",
    "from preprocessing.log_to_history import Log\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def train_fn(model, train_loader, optimizer, device, criterion, label_keys):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"]  # expected dict-like: labels[key] -> tensor\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)  # list/tuple aligned with label_keys\n",
    "\n",
    "        loss = 0.0\n",
    "        for i, key in enumerate(label_keys):\n",
    "            loss = loss + criterion[key](outputs[i].to(device), labels[key].to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "    return total_loss / max(1, len(train_loader))\n",
    "\n",
    "\n",
    "def evaluate_fn(model, data_loader, criterion, device, label_keys):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = 0.0\n",
    "            for i, key in enumerate(label_keys):\n",
    "                loss = loss + criterion[key](outputs[i].to(device), labels[key].to(device))\n",
    "\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "    return total_loss / max(1, len(data_loader))\n",
    "\n",
    "\n",
    "def train_llm(model, train_loader, val_loader, optimizer, epochs, criterion, device, label_keys, patience=10):\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_fn(model, train_loader, optimizer, device, criterion, label_keys)\n",
    "        valid_loss = evaluate_fn(model, val_loader, criterion, device, label_keys)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {valid_loss:.4f}\")\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Validation loss hasn't improved for {patience} epochs. Early stopping...\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "\n",
    "    # dataset / paths\n",
    "    p.add_argument(\"--csv_log\", type=str, default=\"helpdesk\", help=\"dataset/log name (e.g., helpdesk)\")\n",
    "    p.add_argument(\"--type\", type=str, default=\"all\", help=\"TYPE (e.g., all)\")\n",
    "    p.add_argument(\"--semantic_dir\", type=str, default=\"semantic_data\", help=\"base folder of semantic_data\")\n",
    "\n",
    "    # model / training\n",
    "    p.add_argument(\"--model_name\", type=str, default=\"prajjwal1/bert-medium\")\n",
    "    p.add_argument(\"--max_len\", type=int, default=512)\n",
    "    p.add_argument(\"--batch_size\", type=int, default=8)\n",
    "    p.add_argument(\"--lr\", type=float, default=1e-5)\n",
    "    p.add_argument(\"--epochs\", type=int, default=100)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    # runtime\n",
    "    p.add_argument(\"--num_workers\", type=int, default=0)\n",
    "    p.add_argument(\"--multi_gpu\", action=\"store_true\", help=\"use DataParallel when multiple GPUs are available\")\n",
    "\n",
    "    # outputs\n",
    "    p.add_argument(\"--models_dir\", type=str, default=\"models\")\n",
    "    p.add_argument(\"--output_dir\", type=str, default=\"output\")\n",
    "    p.add_argument(\"--early_stop_patience\", type=int, default=10)\n",
    "\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device-->\", device)\n",
    "\n",
    "    # generate semantic data (kept like your script)\n",
    "    Log(args.csv_log, args.type)\n",
    "\n",
    "    base = os.path.join(args.semantic_dir, args.csv_log)\n",
    "\n",
    "    id2label_path = os.path.join(base, f\"{args.csv_log}_id2label_{args.type}.pkl\")\n",
    "    label2id_path = os.path.join(base, f\"{args.csv_log}_label2id_{args.type}.pkl\")\n",
    "    train_path = os.path.join(base, f\"{args.csv_log}_train_{args.type}.pkl\")\n",
    "    y_train_path = os.path.join(base, f\"{args.csv_log}_label_train_{args.type}.pkl\")\n",
    "    y_train_suffix_path = os.path.join(base, f\"{args.csv_log}_suffix_train_{args.type}.pkl\")\n",
    "\n",
    "    id2label = load_pickle(id2label_path)\n",
    "    _label2id = load_pickle(label2id_path)  # loaded but not used in your script (kept)\n",
    "    train = load_pickle(train_path)\n",
    "    _y_train = load_pickle(y_train_path)  # loaded but not used in your script (kept)\n",
    "    y_train_suffix = load_pickle(y_train_suffix_path)\n",
    "\n",
    "    # split inputs\n",
    "    train_input, val_input = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "    # split labels per head (same random_state as your code)\n",
    "    train_label = {}\n",
    "    val_label = {}\n",
    "    for key in y_train_suffix.keys():\n",
    "        train_label[key], val_label[key] = train_test_split(\n",
    "            y_train_suffix[key], test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "    # tokenizer / backbone\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, truncation_side=\"left\")\n",
    "    backbone = AutoModel.from_pretrained(args.model_name)\n",
    "\n",
    "    # keep a stable head order\n",
    "    label_keys = list(train_label.keys())\n",
    "\n",
    "    # your original logic: output size per head == len(id2label['activity'])\n",
    "    # (kept as-is, just expressed consistently)\n",
    "    output_sizes = [len(id2label[\"activity\"]) for _ in label_keys]\n",
    "\n",
    "    train_dataset = CustomDataset(train_input, train_label, tokenizer, args.max_len)\n",
    "    val_dataset = CustomDataset(val_input, val_label, tokenizer, args.max_len)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers\n",
    "    )\n",
    "\n",
    "    model = BertMultiOutputClassificationHeads(backbone, output_sizes).to(device)\n",
    "\n",
    "    # multi-GPU (DataParallel)\n",
    "    if args.multi_gpu and torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        print(f\"Using DataParallel on {torch.cuda.device_count()} GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # criterion per head\n",
    "    criterion = {k: torch.nn.CrossEntropyLoss() for k in label_keys}\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = train_llm(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        epochs=args.epochs,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        label_keys=label_keys,\n",
    "        patience=args.early_stop_patience,\n",
    "    )\n",
    "    exec_time = time.time() - start_time\n",
    "\n",
    "    os.makedirs(args.models_dir, exist_ok=True)\n",
    "    save_path = os.path.join(args.models_dir, f\"{args.csv_log}_{args.type}.pth\")\n",
    "\n",
    "    # DataParallel needs .module for clean state_dict\n",
    "    state_dict = model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict()\n",
    "    torch.save(state_dict, save_path)\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    with open(os.path.join(args.output_dir, f\"{args.csv_log}_{args.type}.txt\"), \"w\") as f:\n",
    "        f.write(str(exec_time))\n",
    "\n",
    "    print(f\"Saved model to: {save_path}\")\n",
    "    print(f\"Execution time (s): {exec_time:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57de70-fbe4-42d2-bdab-1bc6fc84de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from jellyfish import damerau_levenshtein_distance\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from neural_network.HistoryDataset import CustomDataset\n",
    "from neural_network.llamp_multiout import BertMultiOutputClassificationHeads\n",
    "from preprocessing.log_to_history import Log\n",
    "\n",
    "\n",
    "def clean_sequence(sequence_str, label2id):\n",
    "    sequence_list = sequence_str.split(\" \")\n",
    "    end_activity_str = str(label2id[\"activity\"][\"ENDactivity\"])\n",
    "\n",
    "    if end_activity_str in sequence_list:\n",
    "        first_end_index = sequence_list.index(end_activity_str)\n",
    "        sequence_list = sequence_list[: first_end_index + 1]\n",
    "\n",
    "    return \" \".join(sequence_list)\n",
    "\n",
    "\n",
    "def remove_word(sentence, word):\n",
    "    words = sentence.split()\n",
    "    words = [w for w in words if w != word]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def pad_list_to_length(seq, target_length, end_id):\n",
    "    if target_length <= 0:\n",
    "        return seq\n",
    "\n",
    "    seq_len = len(seq)\n",
    "    if seq_len == 0:\n",
    "        return [0] * target_length\n",
    "\n",
    "    if seq_len < target_length:\n",
    "        return seq + [end_id] * (target_length - seq_len)\n",
    "    if seq_len > target_length:\n",
    "        return seq[:target_length]\n",
    "    return seq\n",
    "\n",
    "\n",
    "def extract_prefix(full_trace, suffix_sequence):\n",
    "    pad_token = suffix_sequence[-1]\n",
    "    if pad_token in full_trace:\n",
    "        effective_full = full_trace[: full_trace.index(pad_token)]\n",
    "    else:\n",
    "        effective_full = full_trace\n",
    "\n",
    "    if pad_token in suffix_sequence:\n",
    "        effective_suffix = suffix_sequence[: suffix_sequence.index(pad_token)]\n",
    "    else:\n",
    "        effective_suffix = suffix_sequence\n",
    "\n",
    "    prefix_length = len(effective_full) - len(effective_suffix)\n",
    "    return full_trace[:prefix_length]\n",
    "\n",
    "\n",
    "def predict_suffix_no_freq(model_output):\n",
    "    predicted_no_freq = []\n",
    "    for i in range(len(model_output)):\n",
    "        pred = model_output[i].argmax(dim=1).cpu().numpy()\n",
    "        predicted_no_freq.append(str(pred[0]))\n",
    "    return predicted_no_freq\n",
    "\n",
    "\n",
    "def predict_suffix_with_freq(\n",
    "    model_output,\n",
    "    prefix_sequence,     # list[str]\n",
    "    trace_frequencies,   # dict{tuple(int): freq}\n",
    "    label2id,\n",
    "    beta: float,\n",
    "    threshold: float,\n",
    "):\n",
    "    # Decide fixed padding length\n",
    "    if len(trace_frequencies) == 0:\n",
    "        max_len_in_db = 0\n",
    "    else:\n",
    "        max_len_in_db = max(len(k) for k in trace_frequencies.keys())\n",
    "\n",
    "    # 1) model suffix\n",
    "    model_suffix = []\n",
    "    end_id = label2id[\"activity\"][\"ENDactivity\"]\n",
    "\n",
    "    for step_logits in model_output:\n",
    "        probs = torch.softmax(step_logits, dim=1)[0]\n",
    "        next_act = probs.argmax().item()\n",
    "\n",
    "        if str(next_act) == str(end_id):\n",
    "            break\n",
    "        model_suffix.append(str(next_act))\n",
    "\n",
    "        if str(next_act) == str(end_id):\n",
    "            break\n",
    "\n",
    "    prefix_ints = [int(x) for x in prefix_sequence]\n",
    "    suffix_ints = [int(x) for x in model_suffix]\n",
    "    candidate_trace = prefix_ints + suffix_ints\n",
    "\n",
    "    # 2) pad candidate before exact match\n",
    "    padded_candidate = pad_list_to_length(candidate_trace, max_len_in_db, end_id)\n",
    "    candidate_tuple = tuple(padded_candidate)\n",
    "\n",
    "    # 3) exact match\n",
    "    if candidate_tuple in trace_frequencies:\n",
    "        return model_suffix\n",
    "\n",
    "    # 4) best match by DL + frequency\n",
    "    best_trace = None\n",
    "    best_similarity = -1.0\n",
    "    best_freq = -1.0\n",
    "    best_tau = -1.0\n",
    "\n",
    "    if len(trace_frequencies) == 0 or max_len_in_db == 0:\n",
    "        return model_suffix\n",
    "\n",
    "    candidate_str = \" \".join(map(str, padded_candidate))\n",
    "    f_max = max(trace_frequencies.values())\n",
    "\n",
    "    for hist_trace, freq in trace_frequencies.items():\n",
    "        hist_list = list(hist_trace)\n",
    "        padded_hist = pad_list_to_length(hist_list, max_len_in_db, end_id)\n",
    "        hist_str = \" \".join(map(str, padded_hist))\n",
    "\n",
    "        dl_dist = damerau_levenshtein_distance(candidate_str, hist_str)\n",
    "        similarity = max(0.0, 1.0 - (dl_dist / max_len_in_db))\n",
    "        tau = beta * similarity + (1.0 - beta) * (freq / f_max)\n",
    "\n",
    "        if (\n",
    "            tau > best_tau\n",
    "            or (tau == best_tau and similarity > best_similarity)\n",
    "            or (tau == best_tau and similarity == best_similarity and freq > best_freq)\n",
    "        ):\n",
    "            best_tau = tau\n",
    "            best_similarity = similarity\n",
    "            best_freq = freq\n",
    "            best_trace = hist_list\n",
    "\n",
    "    if best_similarity >= threshold and best_trace is not None and len(best_trace) > len(prefix_ints):\n",
    "        override_suffix_int = best_trace[len(prefix_ints) :]\n",
    "        return list(map(str, override_suffix_int))\n",
    "\n",
    "    return model_suffix\n",
    "\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def predict_next_activities(\n",
    "    model,\n",
    "    test_loader,\n",
    "    label2id,\n",
    "    trace_frequencies,\n",
    "    out_path,\n",
    "    beta,\n",
    "    threshold,\n",
    "    device,\n",
    "):\n",
    "    model.eval()\n",
    "    list_dl_distance_no_freq = []\n",
    "    list_dl_distance_with_freq = []\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, \"w\") as file_dl:\n",
    "        file_dl.write(\n",
    "            \"Prefix,Predicted_NoFreq,Predicted_WithFreq,Truth,\"\n",
    "            \"DL_Score_NoFreq,DL_Score_WithFreq,avg_noFreq,avg_withfreq\\n\"\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                output = model(input_ids, attention_mask)\n",
    "\n",
    "                true_sequence = []\n",
    "                full_trace = []\n",
    "\n",
    "                # 你的数据格式：labels / activities 是可索引序列（按 step）\n",
    "                for i in range(len(batch[\"labels\"])):\n",
    "                    true_sequence.append(str(batch[\"labels\"][i].item()))\n",
    "                    full_trace.append(str(batch[\"activities\"][i].item()))\n",
    "\n",
    "                prefix_sequence = extract_prefix(full_trace, true_sequence)\n",
    "\n",
    "                # 1) no freq\n",
    "                predicted_no_freq = predict_suffix_no_freq(output)\n",
    "                predicted_no_freq = clean_sequence(\" \".join(predicted_no_freq), label2id).split()\n",
    "\n",
    "                # 2) with freq\n",
    "                predicted_with_freq = predict_suffix_with_freq(\n",
    "                    output,\n",
    "                    prefix_sequence,\n",
    "                    trace_frequencies,\n",
    "                    label2id,\n",
    "                    beta=beta,\n",
    "                    threshold=threshold,\n",
    "                )\n",
    "                predicted_with_freq = clean_sequence(\" \".join(predicted_with_freq), label2id).split()\n",
    "\n",
    "                # DL similarity\n",
    "                seq_pred_no_freq = clean_sequence(\" \".join(predicted_no_freq), label2id)\n",
    "                seq_pred_with_freq = clean_sequence(\" \".join(predicted_with_freq), label2id)\n",
    "                seq_true = clean_sequence(\" \".join(map(str, true_sequence)), label2id)\n",
    "                seq_prefix = clean_sequence(\" \".join(map(str, prefix_sequence)), label2id)\n",
    "\n",
    "                end_token = str(label2id[\"activity\"][\"ENDactivity\"])\n",
    "                seq_pred_no_freq = remove_word(seq_pred_no_freq, end_token)\n",
    "                seq_pred_with_freq = remove_word(seq_pred_with_freq, end_token)\n",
    "                seq_true = remove_word(seq_true, end_token)\n",
    "                seq_prefix = remove_word(seq_prefix, end_token)\n",
    "\n",
    "                if seq_pred_no_freq == \"\" and seq_true == \"\":\n",
    "                    seq_pred_no_freq = \"end\"\n",
    "                    seq_true = \"end\"\n",
    "                if seq_pred_with_freq == \"\":\n",
    "                    seq_pred_with_freq = \"end\"\n",
    "\n",
    "                dl_no = 1 - (\n",
    "                    damerau_levenshtein_distance(seq_pred_no_freq, seq_true)\n",
    "                    / max(len(seq_pred_no_freq), len(seq_true))\n",
    "                )\n",
    "                dl_w = 1 - (\n",
    "                    damerau_levenshtein_distance(seq_pred_with_freq, seq_true)\n",
    "                    / max(len(seq_pred_with_freq), len(seq_true))\n",
    "                )\n",
    "\n",
    "                list_dl_distance_no_freq.append(dl_no)\n",
    "                list_dl_distance_with_freq.append(dl_w)\n",
    "\n",
    "                file_dl.write(\n",
    "                    f\"{seq_prefix},{seq_pred_no_freq},{seq_pred_with_freq},{seq_true},\"\n",
    "                    f\"{dl_no:.3f},{dl_w:.3f},\"\n",
    "                    f\"{np.mean(list_dl_distance_no_freq):.3f},{np.mean(list_dl_distance_with_freq):.3f}\\n\"\n",
    "                )\n",
    "\n",
    "    print(f\"Avg DL Similarity (No Frequency): {np.mean(list_dl_distance_no_freq):.3f}\")\n",
    "    print(f\"Avg DL Similarity (With Frequency): {np.mean(list_dl_distance_with_freq):.3f}\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "\n",
    "    p.add_argument(\"--csv_log\", type=str, default=\"helpdesk\")\n",
    "    p.add_argument(\"--type\", type=str, default=\"all\")\n",
    "\n",
    "    p.add_argument(\"--semantic_dir\", type=str, default=\"semantic_data\")\n",
    "    p.add_argument(\"--models_dir\", type=str, default=\"models\")\n",
    "    p.add_argument(\"--output_dir\", type=str, default=\"output\")\n",
    "\n",
    "    p.add_argument(\"--model_name\", type=str, default=\"prajjwal1/bert-medium\")\n",
    "    p.add_argument(\"--max_len\", type=int, default=512)\n",
    "\n",
    "    # required by you\n",
    "    p.add_argument(\"--beta\", type=float, default=0.98)\n",
    "    p.add_argument(\"--threshold\", type=float, default=0.4)\n",
    "\n",
    "    # runtime\n",
    "    p.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    p.add_argument(\"--num_workers\", type=int, default=0)\n",
    "    p.add_argument(\"--multi_gpu\", action=\"store_true\")\n",
    "\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device-->\", device)\n",
    "\n",
    "    # keep your original behavior\n",
    "    Log(args.csv_log, args.type)\n",
    "\n",
    "    base = os.path.join(args.semantic_dir, args.csv_log)\n",
    "\n",
    "    test = load_pickle(os.path.join(base, f\"{args.csv_log}_test_{args.type}.pkl\"))\n",
    "    _y_test = load_pickle(os.path.join(base, f\"{args.csv_log}_label_test_{args.type}.pkl\"))  # kept, unused\n",
    "    id2label = load_pickle(os.path.join(base, f\"{args.csv_log}_id2label_{args.type}.pkl\"))\n",
    "    label2id = load_pickle(os.path.join(base, f\"{args.csv_log}_label2id_{args.type}.pkl\"))\n",
    "    y_train_suffix = load_pickle(os.path.join(base, f\"{args.csv_log}_suffix_train_{args.type}.pkl\"))\n",
    "    y_test_prefix = load_pickle(os.path.join(base, f\"{args.csv_log}_prefixes_test_{args.type}.pkl\"))\n",
    "    y_test_suffix = load_pickle(os.path.join(base, f\"{args.csv_log}_suffix_test_{args.type}.pkl\"))\n",
    "    y_test_activities = load_pickle(os.path.join(base, f\"{args.csv_log}_activities_test_{args.type}.pkl\"))\n",
    "    trace_frequencies = load_pickle(os.path.join(base, f\"{args.csv_log}_encoded_trace_frequencies_{args.type}.pkl\"))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, truncation_side=\"left\")\n",
    "    base_model = AutoModel.from_pretrained(args.model_name)\n",
    "\n",
    "    test_dataset = CustomDataset(\n",
    "        test, y_test_suffix, y_test_prefix, y_test_activities, tokenizer, args.max_len\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers\n",
    "    )\n",
    "\n",
    "    output_sizes = [len(id2label[\"activity\"]) for _ in range(len(y_train_suffix))]\n",
    "    model = BertMultiOutputClassificationHeads(base_model, output_sizes)\n",
    "\n",
    "    model_path = os.path.join(args.models_dir, f\"{args.csv_log}_{args.type}.pth\")\n",
    "    state = torch.load(model_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    if args.multi_gpu and torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        print(f\"Using DataParallel on {torch.cuda.device_count()} GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    out_path = os.path.join(args.output_dir, f\"{args.csv_log}_{args.type}.txt\")\n",
    "    predict_next_activities(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        label2id=label2id,\n",
    "        trace_frequencies=trace_frequencies,\n",
    "        out_path=out_path,\n",
    "        beta=args.beta,\n",
    "        threshold=args.threshold,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
